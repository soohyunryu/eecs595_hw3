{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIUz6Xrk7Mn"
      },
      "source": [
        "# EECS 595 Fall 2022 HW 3\n",
        "\n",
        "Last update: 2022.10.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers, and the resulting pretraining-finetuning paradigm, have revolutionized the field of NLP and many other fields in recent years.\n",
        "\n",
        "[HuggingFaceðŸ¤—](https://huggingface.co/) provides thousands of pretrained Transformer models to perform tasks on different modalities such as text, vision, and audio. We strongly encourage students to explore more [Transformer Tutorials](https://huggingface.co/docs/transformers/notebooks) provided by HuggingFace official, where this assignment is adapted upon."
      ],
      "metadata": {
        "id": "5ER4o2B60_-5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7BQNAknKvsn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great Lakes Setups"
      ],
      "metadata": {
        "id": "TM_bvOXz5tkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the [Great Lakes guide](https://sled-group.github.io/compute-guide/great-lakes)."
      ],
      "metadata": {
        "id": "_azQwMyP552h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WOO9tvye13"
      },
      "source": [
        "### Dependency Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the python version."
      ],
      "metadata": {
        "id": "7IAcmPPiD2vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from platform import python_version\n",
        "assert int(python_version().split(\".\")[1]) >= 5, \\\n",
        "    \"<Warning>: Your Python version is \" + python_version()"
      ],
      "metadata": {
        "id": "AW01VyCMDjs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install [`matplotlib`](https://matplotlib.org/) and formatting helpers."
      ],
      "metadata": {
        "id": "ee4V5MK62uPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.size'] = 16"
      ],
      "metadata": {
        "id": "bOh93Nq_2xrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install standard mathematical and machine learning packages: [`numpy`](https://numpy.org/) and [PyTorch](https://pytorch.org/)."
      ],
      "metadata": {
        "id": "WzUSqbkJEH_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "sB-8oamUDllo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzgCrnTGzNYv"
      },
      "source": [
        "Set the random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tv9jG1yzT5m"
      },
      "source": [
        "import random\n",
        "\n",
        "SEED = 595\n",
        "\n",
        "def set_seed(seed):   \n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install HuggingFace [Transformers](https://pypi.org/project/transformers/), [Datasets](https://huggingface.co/docs/datasets/index), and [Evaluation](https://huggingface.co/docs/evaluate/index)."
      ],
      "metadata": {
        "id": "eOGF8QkUt321"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install datasets\n",
        "! pip install evaluate"
      ],
      "metadata": {
        "id": "_-4Bwhe7twrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Tutorial"
      ],
      "metadata": {
        "id": "IeyrVU4207NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Pipelines\n",
        "\n",
        "[`pipeline()`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) is the easiest way to use a pretrained model for a given task."
      ],
      "metadata": {
        "id": "2MZAqn9MRdFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "id": "mnnt07r_OqtU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "1f983fd4-23ab-4648-98c5-2f25466afbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import `pipeline()` from `transformers`."
      ],
      "metadata": {
        "id": "M1NLYT84UVd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "ot_OY7m4UT-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We simply call `pipeline(\"sentiment-analysis\")`. The default pretrained model, *i.e.*, `distilbert-base-uncased-finetuned-sst-2-english` will be loaded and cached for sentiment analysis."
      ],
      "metadata": {
        "id": "YVplOHTiVp8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "Uvcfodpz1gpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the hundreds of code you wrote in HW1 for sentiment analysis. With the help of `pipeline()`, everything can be done in a few lines:"
      ],
      "metadata": {
        "id": "Hz16AW1u1p2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = classifier([\n",
        "    \"EECS 595 is a great introduction course to natural language processing.\", \n",
        "    \"We hope that this course will deepen your interest in natural language processing.\", \n",
        "    \"We hope that you would not hate the assignments we developed.\", \n",
        "    \"Although some of them could be challenging for students with little background.\",\n",
        "    \"Please feel free to reach out to the teaching team for help.\", \n",
        "])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "metadata": {
        "id": "NO-olcnSUJot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Tokenizers\n",
        "\n",
        "Similar to `nltk`'s tokenizer, we process textual data in HuggingFace using its own [tokenizer](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer). A tokenizer starts by splitting text into *tokens* according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer."
      ],
      "metadata": {
        "id": "P8EGmuCezePX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "IksPoIA50TUR",
        "outputId": "da8f7695-93e7-40ab-8012-ca08ec0183f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get started quickly by loading a pretrained tokenizer with the [`AutoTokenizer`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) class. \n",
        "\n",
        "\n",
        "We start by loading a pretrained tokenizer with [`AutoTokenizer.from_pretrained()`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained). This downloads the *vocab* used when a model is pretrained."
      ],
      "metadata": {
        "id": "edrg9B1_0QBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "ssAlSWdR15dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the [`bert-base-cased`](https://huggingface.co/bert-base-cased) model. Note here that if you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-index mapping (usually referrred to as the *vocab*) during pretraining. This is a problem which some of you encountered in HW2 developing RNN."
      ],
      "metadata": {
        "id": "MyC5TbMV1_JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "AxI8g0GC16o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply pass a sentence to the tokenizer. The tokenizer returns a dictionary with three important itmes:\n",
        "\n",
        "* [`input_ids`](https://huggingface.co/docs/transformers/main/en/glossary#input-ids) are the indices corresponding to each token in the sentence.\n",
        "* [`attention_mask`](https://huggingface.co/docs/transformers/main/en/glossary#attention-mask) indicates whether a token should be attended to or not.\n",
        "* [`token_type_ids`](https://huggingface.co/docs/transformers/main/en/glossary#token-type-ids) identifies which sequence a token belongs to when there is more than one sequence."
      ],
      "metadata": {
        "id": "PoGxt1r83Nqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"There are eight words in this sentence.\")\n",
        "pprint.pprint(encoded_input)"
      ],
      "metadata": {
        "id": "Iwx0ggKE2UG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might noticed that there are 8 words (including punctuation) in the original sentence, but there are 10 indices appearing in the `encoded_input`. You can decode the `input_ids` to return the original input:"
      ],
      "metadata": {
        "id": "faghOcE73QiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(encoded_input[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "bx_IL1XO3d78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. They should look familiar as you have seen similar things in HW2. Not all models need\n",
        "special tokens, but if they do, the tokenizer will automatically add them for you."
      ],
      "metadata": {
        "id": "02r-fjgu39Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in tokenizer.all_special_ids:\n",
        "    print(index, tokenizer.decode(index))"
      ],
      "metadata": {
        "id": "H8YcmYRs44cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are several sentences you want to process, pass the sentences as a list to the tokenizer."
      ],
      "metadata": {
        "id": "AOrt_lfK6jRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"A short sentence.\",\n",
        "    \"This is a longer sentence just for demo pupose.\"\n",
        "]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "pprint.pprint(encoded_inputs)"
      ],
      "metadata": {
        "id": "tOn5Nhtu6mM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to HW2, when you process a batch of sentences, they aren't always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special *padding token* to sentences with fewer tokens. \n",
        "*   Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence. You should see the tokenizer padded the shorter sentences with a `0` because that's the `special_id` of `[PAD]`. \n",
        "\n",
        "On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length. \n",
        "*   Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model.\n",
        "*   In practice, we usually set `padding` to `max_length`."
      ],
      "metadata": {
        "id": "ecaordpg8Klw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = tokenizer(batch_sentences, padding=True, truncation=True)\n",
        "pprint.pprint(encoded_inputs)"
      ],
      "metadata": {
        "id": "pA-4g9D288SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Datasets"
      ],
      "metadata": {
        "id": "_HfeQPsvAktw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "i2bb1cmQm-1w",
        "outputId": "0b95ed39-dd2d-4bb3-b64b-2fed1695b333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_BZearw7f0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will again use the [`rotten_tomatoes`](https://huggingface.co/datasets/rotten_tomatoes) dataset, the same standard movie dataset for sentiment analysis we used in HW1. HuggingFace maintains considerable many public datasets, and fortunately we can just load it from its API."
      ],
      "metadata": {
        "id": "nmM4duxbAoAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "pprint.pprint(dataset)\n",
        "pprint.pprint(dataset['train'][0])"
      ],
      "metadata": {
        "id": "4z5MMBz6CPil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you now know, you need a tokenizer to process the text and include a padding and truncation strategy to handle any variable sequence lengths. To process your dataset in one step, use the [`map()`](https://huggingface.co/docs/datasets/process.html#map) method to apply a preprocessing function over the entire dataset:"
      ],
      "metadata": {
        "id": "3ybBkvvAuCwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "ck1A0p0RC1xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class. But for the purpose of exercise (and prepare you for more flexible use of HuggingFace), you will manually postprocess `tokenized_dataset` to prepare it for training."
      ],
      "metadata": {
        "id": "FhppBEu8uNlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "c-u-yi26ZlNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `DataLoader` for your training and test datasets so you can iterate over batches of data. The batch size is a critical parameter to tune. Read more about it [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance), especially when you encounter `RuntimeError: CUDA error: out of memory`."
      ],
      "metadata": {
        "id": "ovly5sZQm40h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=SEED)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=SEED)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-wKR-0FJZsB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Fine-tuning"
      ],
      "metadata": {
        "id": "uG8_fm0Gnepp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `bert-base-cased` model with the number of expected labels. In this case of binary classification, set `num_labels` to 2."
      ],
      "metadata": {
        "id": "q9WEbXLJvLjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6DKNtrmZbLfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an optimizer and learning rate scheduler to fine-tune the model. \n",
        "*   Use the [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer; \n",
        "*   Create the default learning rate scheduler from [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer).\n",
        "\n",
        "You may want to do some fine-tuning here!"
      ],
      "metadata": {
        "id": "LAYVTlCev0H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", \n",
        "    optimizer=optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "VpGm2TARekXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2022, HuggingFace updated the `evaluate` APIs. Read more about the details [here](https://www.vennify.ai/hugging-face-evaluate-library/)."
      ],
      "metadata": {
        "id": "bAfs8HvPwIA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "YJKAwGPSwZME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your training progress, use the [tqdm](https://tqdm.github.io/) library to add a progress bar over the number of training steps. Just like how you need to add an evaluation function to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), you need to do the same when you write your own training loop. But instead of calculating and reporting the metric at the end of each epoch, this time you will accumulate all the batches with [`add_batch`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=add_batch#datasets.Metric.add_batch) and calculate the metric at the very end."
      ],
      "metadata": {
        "id": "0ATxx-lxw6MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "    \n",
        "    score = metric.compute()\n",
        "    print('Validation Accuracy:', score['accuracy'])"
      ],
      "metadata": {
        "id": "KpyQy2IaerIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model on the test set, and save the predictions for grading purposes."
      ],
      "metadata": {
        "id": "gH2-qdsxxAC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "model.eval()\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "torch.save(predictions, 'predictions.torch')\n",
        "score = metric.compute()\n",
        "print('Test Accuracy:', score['accuracy'])"
      ],
      "metadata": {
        "id": "zasVKC_AbdDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to HTML\n",
        "\n",
        "Download this notebook, re-upload it to the `Files` on the left, and run the following cell."
      ],
      "metadata": {
        "id": "WD1pdrQnOSvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html hw3.ipynb"
      ],
      "metadata": {
        "id": "J-PiZr0WORye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60130ed-5b50-4247-db20-d5ca27229985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook hw3.ipynb to html\n",
            "[NbConvertApp] Writing 322200 bytes to hw3.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}